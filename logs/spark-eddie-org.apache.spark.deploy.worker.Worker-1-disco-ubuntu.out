Spark Command: /usr/lib/jvm/jdk1.8.0_111/bin/java -cp /home/eddie/frameworks/spark-2.0.1-compile/conf/:/home/eddie/frameworks/spark-2.0.1-compile/assembly/target/scala-2.11/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://disco-ubuntu:7077
========================================
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/11/03 15:37:37 INFO Worker: Started daemon with process name: 2514@disco-ubuntu
16/11/03 15:37:37 INFO SignalUtils: Registered signal handler for TERM
16/11/03 15:37:37 INFO SignalUtils: Registered signal handler for HUP
16/11/03 15:37:37 INFO SignalUtils: Registered signal handler for INT
16/11/03 15:37:37 WARN Utils: Your hostname, disco-ubuntu resolves to a loopback address: 127.0.1.1; using 128.198.181.131 instead (on interface enp0s25)
16/11/03 15:37:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/11/03 15:37:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/11/03 15:37:37 INFO SecurityManager: Changing view acls to: eddie
16/11/03 15:37:37 INFO SecurityManager: Changing modify acls to: eddie
16/11/03 15:37:37 INFO SecurityManager: Changing view acls groups to: 
16/11/03 15:37:37 INFO SecurityManager: Changing modify acls groups to: 
16/11/03 15:37:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(eddie); groups with view permissions: Set(); users  with modify permissions: Set(eddie); groups with modify permissions: Set()
16/11/03 15:37:37 INFO Utils: Successfully started service 'sparkWorker' on port 38826.
16/11/03 15:37:38 INFO Worker: Starting Spark worker 128.198.181.131:38826 with 4 cores, 2.7 GB RAM
16/11/03 15:37:38 INFO Worker: Running Spark version 2.0.1
16/11/03 15:37:38 INFO Worker: Spark home: /home/eddie/frameworks/spark-2.0.1-compile
16/11/03 15:37:38 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
16/11/03 15:37:38 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://128.198.181.131:8081
16/11/03 15:37:38 INFO Worker: Connecting to master disco-ubuntu:7077...
16/11/03 15:37:38 INFO TransportClientFactory: Successfully created connection to disco-ubuntu/127.0.1.1:7077 after 19 ms (0 ms spent in bootstraps)
16/11/03 15:37:38 INFO Worker: Successfully registered with master spark://disco-ubuntu:7077
16/11/03 15:37:43 INFO Worker: Asked to launch executor app-20161103153743-0001/0 for Spark Pi
16/11/03 15:37:43 INFO SecurityManager: Changing view acls to: eddie
16/11/03 15:37:43 INFO SecurityManager: Changing modify acls to: eddie
16/11/03 15:37:43 INFO SecurityManager: Changing view acls groups to: 
16/11/03 15:37:43 INFO SecurityManager: Changing modify acls groups to: 
16/11/03 15:37:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(eddie); groups with view permissions: Set(); users  with modify permissions: Set(eddie); groups with modify permissions: Set()
16/11/03 15:37:43 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/jdk1.8.0_111/bin/java" "-cp" "/home/eddie/frameworks/spark-2.0.1-compile/conf/:/home/eddie/frameworks/spark-2.0.1-compile/assembly/target/scala-2.11/jars/*" "-Xmx1024M" "-Dspark.driver.port=37218" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@128.198.181.131:37218" "--executor-id" "0" "--hostname" "128.198.181.131" "--cores" "4" "--app-id" "app-20161103153743-0001" "--worker-url" "spark://Worker@128.198.181.131:38826"
16/11/03 15:37:47 INFO Worker: Asked to kill executor app-20161103153743-0001/0
16/11/03 15:37:47 INFO ExecutorRunner: Runner thread for executor app-20161103153743-0001/0 interrupted
16/11/03 15:37:47 INFO ExecutorRunner: Killing process!
16/11/03 15:37:47 INFO Worker: Executor app-20161103153743-0001/0 finished with state KILLED exitStatus 143
16/11/03 15:37:47 INFO Worker: Cleaning up local directories for application app-20161103153743-0001
16/11/03 15:37:47 INFO ExternalShuffleBlockResolver: Application app-20161103153743-0001 removed, cleanupLocalDirs = true
16/11/03 16:21:12 INFO Worker: disco-ubuntu:7077 Disassociated !
16/11/03 16:21:13 ERROR Worker: Connection to master failed! Waiting for master to reconnect...
16/11/03 16:21:13 INFO Worker: disco-ubuntu:7077 Disassociated !
16/11/03 16:21:13 ERROR Worker: Connection to master failed! Waiting for master to reconnect...
16/11/03 16:21:13 INFO Worker: Connecting to master disco-ubuntu:7077...
16/11/03 16:21:14 INFO Worker: Not spawning another attempt to register with the master, since there is an attempt scheduled already.
16/11/03 16:21:15 INFO TransportClientFactory: Found inactive connection to disco-ubuntu/127.0.1.1:7077, creating a new one.
16/11/03 16:21:18 WARN Worker: Failed to connect to master disco-ubuntu:7077
org.apache.spark.SparkException: Exception thrown in awaitResult
	at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:77)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:75)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:83)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:88)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:96)
	at org.apache.spark.deploy.worker.Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$tryRegisterAllMasters$1$$anon$1.run(Worker.scala:216)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to disco-ubuntu/127.0.1.1:7077
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:228)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:179)
	at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:197)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:191)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:187)
	... 4 more
Caused by: java.net.ConnectException: Connection refused: disco-ubuntu/127.0.1.1:7077
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more
16/11/03 16:21:20 INFO Worker: Retrying connection to master (attempt # 1)
16/11/03 16:21:20 INFO Worker: Connecting to master disco-ubuntu:7077...
16/11/03 16:21:20 INFO TransportClientFactory: Found inactive connection to disco-ubuntu/127.0.1.1:7077, creating a new one.
16/11/03 16:21:20 WARN Worker: Failed to connect to master disco-ubuntu:7077
org.apache.spark.SparkException: Exception thrown in awaitResult
	at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:77)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:75)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:83)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:88)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:96)
	at org.apache.spark.deploy.worker.Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$reregisterWithMaster$1$$anon$2.run(Worker.scala:272)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to disco-ubuntu/127.0.1.1:7077
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:228)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:179)
	at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:197)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:191)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:187)
	... 4 more
Caused by: java.net.ConnectException: Connection refused: disco-ubuntu/127.0.1.1:7077
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more
16/11/03 16:21:25 INFO Worker: Retrying connection to master (attempt # 2)
16/11/03 16:21:25 INFO Worker: Connecting to master disco-ubuntu:7077...
16/11/03 16:21:25 INFO TransportClientFactory: Found inactive connection to disco-ubuntu/127.0.1.1:7077, creating a new one.
16/11/03 16:21:26 WARN Worker: Failed to connect to master disco-ubuntu:7077
org.apache.spark.SparkException: Exception thrown in awaitResult
	at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:77)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:75)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:83)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:88)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:96)
	at org.apache.spark.deploy.worker.Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$reregisterWithMaster$1$$anon$2.run(Worker.scala:272)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to disco-ubuntu/127.0.1.1:7077
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:228)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:179)
	at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:197)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:191)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:187)
	... 4 more
Caused by: java.net.ConnectException: Connection refused: disco-ubuntu/127.0.1.1:7077
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more
16/11/03 16:21:31 ERROR Worker: RECEIVED SIGNAL TERM
16/11/03 16:21:31 INFO ShutdownHookManager: Shutdown hook called
16/11/03 16:21:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-524e40b6-52cf-4f1f-ac4f-8f131f91be49
